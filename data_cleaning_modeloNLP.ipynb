{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import RFE\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import sys\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "from os.path import join\n",
    "from random import shuffle, sample\n",
    "from sklearn.utils import shuffle\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SIRVE PARA TOKENIZAR\n",
    "def to_single_message_format(gamefile):\n",
    "    messages = []\n",
    "    with open(gamefile) as inh:\n",
    "        for ln in inh:\n",
    "            conversation = json.loads(ln)\n",
    "            for msg, sender_label, receiver_label, score_delta \\\n",
    "                in zip(conversation['messages'],conversation['sender_labels'], \\\n",
    "                    conversation['receiver_labels'], conversation['game_score_delta']):\n",
    "                messages.append({'message': msg, 'receiver_annotation': receiver_label,\\\n",
    "                    'sender_annotation':sender_label, 'score_delta': int(score_delta)})\n",
    "    shuffle(messages)    \n",
    "    return messages\n",
    "\n",
    "def write_single_messages(messages, outfile):\n",
    "    with open(outfile, \"w\") as outh:\n",
    "        for msg in messages:\n",
    "            outh.write(json.dumps(msg)+'\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ROOT = 'data/'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train (13132,)\n",
      "Distribuci贸n de clases en el conjunto de entrenamiento:\n",
      "True     12541\n",
      "False      591\n",
      "Name: sender_annotation, dtype: int64\n",
      "Distribuci贸n de clases en el conjunto de test:\n",
      "True     2501\n",
      "False     240\n",
      "Name: sender_annotation, dtype: int64\n",
      "Distribuci贸n de clases en el conjunto de validaci贸n:\n",
      "True     1360\n",
      "False      56\n",
      "Name: sender_annotation, dtype: int64\n",
      "Distribuci贸n de la clases en el nuevo conjunto de entrenamiento:\n",
      "True     12541\n",
      "False    12541\n",
      "Name: sender_annotation, dtype: int64\n",
      "Y_train nuevo (25082,)\n",
      "Dimensiones de df_train_copy: (25082, 4)\n",
      "Dimensiones de df_test_copy: (2741, 4)\n",
      "Dimensiones de df_validation_copy: (1416, 4)\n",
      "Dimensiones de y_train: (25082,)\n",
      "*******************************:\n",
      "Dimensiones de df_train: (25082, 4)\n"
     ]
    }
   ],
   "source": [
    "#LECTURA DE DATOS\n",
    "df_train = to_single_message_format(join(ROOT, 'train.jsonl'))\n",
    "df_test = to_single_message_format(join(ROOT, 'test.jsonl'))\n",
    "df_validation = to_single_message_format(join(ROOT, 'validation.jsonl'))\n",
    "#TRANSFORMACION A DATAFRAME\n",
    "df_train_copy = pd.DataFrame(df_train)\n",
    "df_test_copy = pd.DataFrame(df_test)\n",
    "df_validation_copy = pd.DataFrame(df_validation)\n",
    "y_train = df_train_copy['sender_annotation']\n",
    "print('Y_train',y_train.shape)\n",
    "\n",
    "# Revisar la distribuci贸n de clases en el conjunto de entrenamiento\n",
    "print(\"Distribuci贸n de clases en el conjunto de entrenamiento:\")\n",
    "print(df_train_copy['sender_annotation'].value_counts())\n",
    "\n",
    "# Revisar la distribuci贸n de clases en el conjunto de validaci贸n\n",
    "print(\"Distribuci贸n de clases en el conjunto de test:\")\n",
    "print(df_test_copy['sender_annotation'].value_counts())\n",
    "\n",
    "# Revisar la distribuci贸n de clases en el conjunto de validaci贸n\n",
    "print(\"Distribuci贸n de clases en el conjunto de validaci贸n:\")\n",
    "print(df_validation_copy['sender_annotation'].value_counts())\n",
    "\n",
    "\n",
    "#EXISTE UN DESBALANCEO DE CLASES, POR LO QUE SE DEBE HACER UN SOBREMUESTREO DE LA CLASE MINORITARIA\n",
    "# Separar los datos por clases\n",
    "df_majority = df_train_copy[df_train_copy['sender_annotation'] == True]\n",
    "df_minority = df_train_copy[df_train_copy['sender_annotation'] == False]\n",
    "# Sobremuestrear la clase minoritaria\n",
    "df_minority_oversampled = df_minority.sample(len(df_majority), replace=True)  # Replicar los datos de la clase minoritaria\n",
    "# Combinar la clase mayoritaria con la clase minoritaria sobremuestreada\n",
    "df_oversampled = pd.concat([df_majority, df_minority_oversampled])\n",
    "# Barajar o sedordena los datos\n",
    "df_oversampled = shuffle(df_oversampled) \n",
    "\n",
    "# Ahora puedes proceder a tokenizar y preparar estos datos para el entrenamiento como lo hiciste anteriormente\n",
    "# Revisar la nueva distribuci贸n de clases en el conjunto de entrenamiento\n",
    "print(\"Distribuci贸n de la clases en el nuevo conjunto de entrenamiento:\")\n",
    "print(df_oversampled['sender_annotation'].value_counts())\n",
    "\n",
    "df_train_copy = df_oversampled\n",
    "y_train = df_train_copy['sender_annotation']\n",
    "print('Y_train nuevo',y_train.shape)\n",
    "\n",
    "\n",
    "print(\"Dimensiones de df_train_copy:\", df_train_copy.shape)\n",
    "print(\"Dimensiones de df_test_copy:\", df_test_copy.shape)\n",
    "print(\"Dimensiones de df_validation_copy:\", df_validation_copy.shape)\n",
    "print(\"Dimensiones de y_train:\", y_train.shape)\n",
    "print(\"*******************************:\")\n",
    "print(\"Dimensiones de df_train:\", df_train_copy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Germany!\\n\\nJust the person I want to speak with. I have a somewhat crazy idea that Ive always wanted to try with I/G, but Ive never actually convinced the other guy to try it. And, whats worse, it might make you suspicious of me. \\n\\nSo...do I suggest it?\\n\\nIm thinking that this is a low stakes game, not a tournament or anything, and an interesting and unusual move set might make it more fun? Thats my hope anyway.\\n\\nWhat is your appetite like for unusual and crazy?',\n",
       " 'receiver_annotation': True,\n",
       " 'sender_annotation': True,\n",
       " 'score_delta': 0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train_copy[0]\n",
    "#df_test_copy[0]\n",
    "#df_train_copy\n",
    "df_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREACION DEL MODELO\n",
    "from transformers import BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de input_ids: torch.Size([25082, 256])\n",
      "Dimensiones de attention_masks: torch.Size([25082, 256])\n",
      "Dimensiones de y_train_tensor: (25082,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_messages(messages, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for message in messages:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            message,                      \n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Concatenar los tensores\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# Aplicar la funci贸n de codificaci贸n a tus mensajes\n",
    "messages = df_train_copy['message'].tolist()\n",
    "input_ids, attention_masks = encode_messages(messages, max_length=256)\n",
    "\n",
    "# Comprobaci贸n de las dimensiones\n",
    "print(\"Dimensiones de input_ids:\", input_ids.shape)\n",
    "print(\"Dimensiones de attention_masks:\", attention_masks.shape)\n",
    "print(\"Dimensiones de y_train_tensor:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "False\n",
      "<class 'list'>\n",
      "True\n",
      "[\"{'message': 'Germany!\\\\n\\\\nJust the person I want to speak with. I have a somewhat crazy idea that Ive always wanted to try with I/G, but Ive never actually convinced the other guy to try it. And, whats worse, it might make you suspicious of me. \\\\n\\\\nSo...do I suggest it?\\\\n\\\\nIm thinking that this is a low stakes game, not a tournament or anything, and an interesting and unusual move set might make it more fun? Thats my hope anyway.\\\\n\\\\nWhat is your appetite like for unusual and crazy?', 'receiver_annotation': True, 'sender_annotation': True, 'score_delta': 0}\", '{\\'message\\': \"You\\'ve whet my appetite, Italy. What\\'s the suggestion?\", \\'receiver_annotation\\': True, \\'sender_annotation\\': True, \\'score_delta\\': 0}', \"{'message': '', 'receiver_annotation': True, 'sender_annotation': True, 'score_delta': 0}\", '{\\'message\\': \"It seems like there are a lot of ways that could go wrong...I don\\'t see why France would see you approaching/taking Munich--while I do nothing about it--and not immediately feel skittish\", \\'receiver_annotation\\': True, \\'sender_annotation\\': True, \\'score_delta\\': 0}', \"{'message': 'Yeah, I cant say Ive tried it and it works, cause Ive never tried it or seen it. But how I think it would work is (a) my Spring move looks like an attack on Austria, so it would not be surprising if you did not cover Munich. Then (b) you build two armies, which looks like were really at war and youre going to eject me. Then we launch the attack in Spring. So there is really no part of this that would raise alarm bells with France.\\\\n\\\\nAll that said, Ive literally never done it before, and it does involve risk for you, so Im not offended or concerned if its just not for you. Im happy to play more conventionally too. Up to you.', 'receiver_annotation': 'NOANNOTATION', 'sender_annotation': True, 'score_delta': 0}\"]\n"
     ]
    }
   ],
   "source": [
    "print(type(df_train))  # Deber铆a ser <class 'list'>\n",
    "print(all(isinstance(item, str) for item in df_train))  # Todos los elementos deben ser cadenas de texto\n",
    "df_train = [str(message) for message in df_train]  # Convertir todos los elementos a cadenas de texto\n",
    "print(type(df_train))  # Deber铆a ser <class 'list'>\n",
    "print(all(isinstance(item, str) for item in df_train))  # Todos los elementos deben ser cadenas de texto\n",
    "print(df_train[:5])  # Imprime los primeros 5 elementos para verificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de input_ids: torch.Size([25082, 256])\n",
      "Dimensiones de attention_masks: torch.Size([25082, 256])\n",
      "Dimensiones de y_train_tensor: torch.Size([25082])\n"
     ]
    }
   ],
   "source": [
    "y_train\n",
    "# Si y_train es una serie de Pandas (por ejemplo, una columna de DataFrame)\n",
    "y_train_tensor = torch.tensor(y_train.values).long()\n",
    "y_train_tensor.shape\n",
    "\n",
    "print(\"Dimensiones de input_ids:\", input_ids.shape)\n",
    "print(\"Dimensiones de attention_masks:\", attention_masks.shape)\n",
    "print(\"Dimensiones de y_train_tensor:\", y_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorDataset\n",
    "#es una clase de PyTorch que encapsula tensores en un conjunto de datos. \n",
    "#En tu caso, estos tensores ser谩n los input_ids y attention_masks generados por el tokenizador,\n",
    "#as铆 como las etiquetas de tus datos (y_train).\n",
    "from torch.utils.data import TensorDataset\n",
    "train_dataset = TensorDataset(input_ids, attention_masks, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataLoader\n",
    "#DataLoader es una clase que proporciona un iterable sobre el conjunto de datos. Con DataLoader, puedes especificar \n",
    "#el tama帽o del lote (batch size), si los datos deben ser mezclados, y otros par谩metros que son 煤tiles durante el entrenamiento.\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de train_dataloader: <torch.utils.data.dataset.TensorDataset object at 0x0000025DFFF52410>\n",
      "Dimensiones de train_dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000025DFFD78190>\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensiones de train_dataloader:\", train_dataset)\n",
    "print(\"Dimensiones de train_dataloader:\", train_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n",
      "NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())  # Deber铆a ser True si CUDA est谩 disponible\n",
    "print(torch.version.cuda)        # Deber铆a mostrar la versi贸n de CUDA\n",
    "print(torch.cuda.get_device_name(0))  # Muestra el nombre de tu GPU CUDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n"
     ]
    }
   ],
   "source": [
    "#CONFIGURACION DEL MODELO BERT\n",
    "import torch\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible. Dispositivos CUDA: 1\n",
      "Nombre del dispositivo CUDA: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#C:\\Users\\nobce\\AppData\\Local\\Temp\\cuda\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA disponible. Dispositivos CUDA:\", torch.cuda.device_count())\n",
    "    print(\"Nombre del dispositivo CUDA:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA no est谩 disponible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Cargar el modelo preentrenado de BERT\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', # Usa la versi贸n base de BERT\n",
    "    num_labels = 2,     # N煤mero de etiquetas de salida (por ejemplo, 2 para clasificaci贸n binaria)\n",
    "    output_attentions = False, # Si el modelo debe retornar las atenciones\n",
    "    output_hidden_states = False, # Si el modelo debe retornar todos los estados ocultos\n",
    ")\n",
    "\n",
    "# Mover el modelo al dispositivo adecuado (GPU o CPU)\n",
    "model.to(device)  # Ejemplo: device puede ser 'cuda' para GPU o 'cpu' para CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    # Mover el lote al dispositivo\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_input_ids = b_input_ids.to(device)\n",
    "    b_input_mask = b_input_mask.to(device)\n",
    "    b_labels = b_labels.to(device)\n",
    "\n",
    "    # Realizar pasos de entrenamiento...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el optimizador y la Funci贸n de P茅rdida\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # tasa de aprendizaje\n",
    "                  eps = 1e-8 # epsilon\n",
    "                 )\n",
    "\n",
    "# Definir la funci贸n de p茅rdida\n",
    "from torch.nn import CrossEntropyLoss\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.24340122679013246\n",
      "Average training loss: 0.03561993770310845\n"
     ]
    }
   ],
   "source": [
    "# N煤mero de 茅pocas de entrenamiento\n",
    "epochs = 2\n",
    "\n",
    "# Bucle para cada 茅poca\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Entrenamiento\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # Desempacar el lote del dataloader y cargarlo al dispositivo adecuado\n",
    "        b_input_ids, b_input_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Limpiar gradientes existentes\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Ejecutar el modelo para obtener logits\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        # Calcular la p茅rdida\n",
    "        loss = loss_fn(outputs.logits, b_labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Retropropagaci贸n para calcular gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizar los par谩metros del modelo\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calcular la p茅rdida promedio sobre la 茅poca\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Average training loss: {avg_train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\nobce\\\\OneDrive\\\\Documentos\\\\PRUEBA CIENCITIFICO DATOS\\\\2024_acl_diplomacy-master\\\\modelo_de_entrenamiento_guardado\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\nobce\\\\OneDrive\\\\Documentos\\\\PRUEBA CIENCITIFICO DATOS\\\\2024_acl_diplomacy-master\\\\modelo_de_entrenamiento_guardado\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\nobce\\\\OneDrive\\\\Documentos\\\\PRUEBA CIENCITIFICO DATOS\\\\2024_acl_diplomacy-master\\\\modelo_de_entrenamiento_guardado\\\\vocab.txt',\n",
       " 'C:\\\\Users\\\\nobce\\\\OneDrive\\\\Documentos\\\\PRUEBA CIENCITIFICO DATOS\\\\2024_acl_diplomacy-master\\\\modelo_de_entrenamiento_guardado\\\\added_tokens.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GUARDA EL MODELO DE ENTRENAMIENTO\n",
    "model.save_pretrained(r\"C:\\Users\\nobce\\OneDrive\\Documentos\\PRUEBA CIENCITIFICO DATOS\\2024_acl_diplomacy-master\\modelo_de_entrenamiento_guardado\")\n",
    "tokenizer.save_pretrained(r\"C:\\Users\\nobce\\OneDrive\\Documentos\\PRUEBA CIENCITIFICO DATOS\\2024_acl_diplomacy-master\\modelo_de_entrenamiento_guardado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizamos los mensajes de la data de validaci贸n\n",
    "validation_inputs, validation_masks = encode_messages(df_validation_copy['message'].tolist(), max_length=256)\n",
    "# Se convierte las etiquetas en tensores\n",
    "validation_labels = torch.tensor(df_validation_copy['sender_annotation'].values).long()  # Aseg煤rate de que 'label' es el nombre de tu columna de etiquetas\n",
    "\n",
    "# Crea el TensorDataset\n",
    "validation_dataset = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "# Crea el DataLoader\n",
    "batch_size = 16  # Puedes ajustar esto seg煤n las necesidades de tu modelo y hardware\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.10      0.16      0.12        56\n",
      "           1       0.96      0.94      0.95      1360\n",
      "\n",
      "    accuracy                           0.91      1416\n",
      "   macro avg       0.53      0.55      0.54      1416\n",
      "weighted avg       0.93      0.91      0.92      1416\n",
      "\n",
      "Confusion Matrix:\n",
      "[[   9   47]\n",
      " [  80 1280]]\n",
      "Accuracy Score:\n",
      "0.9103107344632768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Funci贸n para evaluar el modelo\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()  # Poner el modelo en modo de evaluaci贸n\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            b_input_ids = b_input_ids.to(device)\n",
    "            b_input_mask = b_input_mask.to(device)\n",
    "            b_labels = b_labels.to(device)\n",
    "\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            logits = outputs.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "\n",
    "    flat_predictions = np.concatenate(predictions, axis=0)\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "    flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    return flat_predictions, flat_true_labels\n",
    "\n",
    "# Asumiendo que tienes un DataLoader para tus datos de validaci贸n/prueba\n",
    "predictions, true_labels = evaluate_model(model, validation_dataloader, device)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predictions))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, predictions))\n",
    "\n",
    "print(\"Accuracy Score:\")\n",
    "print(accuracy_score(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segun nuestro modelo, el accuracy es de 0.93, lo cual es bastante bueno, pero no es suficiente para nuestro modelo\n",
    "#lo cual es aconsajable ajustar learning rate o los epochs con mas tiempo \n",
    "\n",
    "#PRUEBA SOBRE EL CONJUNTO DE DATOS DE PRUEBA df_test_copy\n",
    "\n",
    "# Tokenizar los textos de prueba\n",
    "test_inputs, test_masks = encode_messages(df_test_copy['message'].tolist(), max_length=256)\n",
    "# Convertir las etiquetas de prueba en tensores\n",
    "test_labels = torch.tensor(df_test_copy['sender_annotation'].values).long()\n",
    "# Crear el TensorDataset\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "# Crear el DataLoader\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)  # Ajusta el batch_size seg煤n sea necesario\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.12      0.13       240\n",
      "           1       0.92      0.93      0.93      2501\n",
      "\n",
      "    accuracy                           0.86      2741\n",
      "   macro avg       0.53      0.53      0.53      2741\n",
      "weighted avg       0.85      0.86      0.86      2741\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  28  212]\n",
      " [ 165 2336]]\n",
      "Accuracy Score:\n",
      "0.8624589565851879\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo con los datos nuevos que son los datos de df_test_copy\n",
    "test_predictions, test_true_labels = evaluate_model(model, test_dataloader, device)\n",
    "\n",
    "# Imprimir el reporte de clasificaci贸n y la matriz de confusi贸n\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(test_true_labels, test_predictions))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(test_true_labels, test_predictions))\n",
    "\n",
    "print(\"Accuracy Score:\")\n",
    "print(accuracy_score(test_true_labels, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
